{
 "metadata": {
  "name": "",
  "signature": "sha256:73ef52a8d659bb9a79b004e7973f34cedee67090174078f40b2c8fa1f4e798a6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Standard Libraries\n",
      "import cPickle\n",
      "import gzip\n",
      "\n",
      "# External Libraries\n",
      "import numpy as np\n",
      "import theano\n",
      "import theano.tensor as Tensor\n",
      "from theano.tensor.nnet import conv\n",
      "from theano.tensor.nnet import softmax\n",
      "from theano.tensor import shared_randomstreams\n",
      "from theano.tensor.signal import downsample"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Using gpu device 0: Tesla K40c (CNMeM is enabled with initial size: 70.0% of memory, cuDNN 5103)\n",
        "/usr/local/lib/python2.7/dist-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
        "  warnings.warn(warn)\n",
        "/usr/local/lib/python2.7/dist-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
        "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Define Activation functions\n",
      "# Linear Neuron\n",
      "def linear(z):\n",
      "    return z\n",
      "\n",
      "# Rectified Linear Unit\n",
      "def ReLu(z):\n",
      "    return Tensor.maximum(0.0, z)\n",
      "\n",
      "# Tanh Neuron\n",
      "def tanh(z):\n",
      "    return Tensor.tanh(z)\n",
      "\n",
      "# Sigmoid Function\n",
      "def sigmoid(z):\n",
      "    return 1.0/(1+np.exp(-z))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "GPU = True\n",
      "if GPU:\n",
      "    print 'Trying to run under a GPU. If this is not required, '+\\\n",
      "            'Set GPU flag to False'\n",
      "    theano.config.mode = 'FAST_RUN'\n",
      "    try:\n",
      "        theano.config.device = 'gpu'\n",
      "    except:\n",
      "        pass # Already set\n",
      "    theano.config.floatX = 'float32'\n",
      "else:\n",
      "    print 'Runinng under CPU. Inorder run on a GPU Set GPU flag to True'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Trying to run under a GPU. If this is not required, Set GPU flag to False\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load MNIST Data\n",
      "def load_data_shared(filename=\"../data/mnist.pkl.gz\"):\n",
      "    fp = gzip.open(filename, 'rb')\n",
      "    training_data, validation_data, test_data = cPickle.load(fp)\n",
      "    fp.close()\n",
      "    return [shared(training_data), shared(validation_data), shared(test_data)]\n",
      "    \n",
      "def shared(data):\n",
      "    ''' Shared data for Theano library. It places the data onto a GPU\n",
      "    if available'''\n",
      "    shared_x = theano.shared(np.asarray(data[0], dtype=theano.config.floatX),\n",
      "                             borrow=True)\n",
      "    shared_y = theano.shared(np.asarray(data[1], dtype=theano.config.floatX),\n",
      "                             borrow=True)\n",
      "    return shared_x, Tensor.cast(shared_y, \"int32\")\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Main class for the network\n",
      "# Neural Network\n",
      "class Network(object):\n",
      "    def __init__(self, layers, mini_batch_size):\n",
      "        ''' Take a list of 'layers' to construct the network and the \n",
      "        value for 'mini_batch_size' to run stochastic gradient descent\n",
      "        '''\n",
      "        \n",
      "        self.layers = layers\n",
      "        self.mini_batch_size = mini_batch_size\n",
      "        self.parameters = [ parameter for layer in self.layers for parameter in layer.parameters]\n",
      "        self.x = Tensor.matrix('x')\n",
      "        self.y = Tensor.ivector('y')\n",
      "        init_layer = self.layers[0]\n",
      "        init_layer.set_input(self.x, self.x, self.mini_batch_size)\n",
      "        for num in xrange(1, len(self.layers)):\n",
      "            prev_layer, layer = self.layers[num-1], self.layers[num]\n",
      "            layer.set_input(prev_layer.output, prev_layer.output_dropout,\n",
      "                           self.mini_batch_size)\n",
      "        self.output = self.layers[-1].output\n",
      "        self.output_dropout = self.layers[-1].output_dropout\n",
      "        \n",
      "    # Let us define the Stochastic Gradient Descent method.\n",
      "    # The net uses SGD to learn the weights and biases\n",
      "    def SGD(self, training_data, epochs, mini_batch_size, eta, \n",
      "            validation_data, test_data, lmda=0.0):\n",
      "        ''' \n",
      "            Trains the neural network using mini-batch stochastic gradient descent.\n",
      "            The training data is list a tuples (x,y) representing the training input and desired outputs\n",
      "            If validation data is provided the network will be evaluated against after each epoch,\n",
      "        '''\n",
      "        training_x, training_y = training_data\n",
      "        validation_x, validation_y = validation_data\n",
      "        test_x, test_y = test_data\n",
      "        \n",
      "        # Compute the number of batches for training, validation, test\n",
      "        num_training_batches = size(training_data)/mini_batch_size\n",
      "        num_validation_batches = size(validation_data)/mini_batch_size\n",
      "        num_test_batches = size(test_data)/mini_batch_size\n",
      "        \n",
      "        # Define the cost (regularized) function, gradients, updates\n",
      "        l2_norm_squared = sum([(layer.weights**2).sum() for layer in self.layers])\n",
      "        cost = self.layers[-1].cost(self) +\\\n",
      "            0.5*lmda*l2_norm_squared/num_training_batches\n",
      "        gradients = Tensor.grad(cost, self.parameters)\n",
      "        updates = [(parameter, parameter - eta*gradient)\n",
      "                   for parameter, gradient in zip(self.parameters,gradients)]\n",
      "        \n",
      "        # Functions to train the mini batch and to compute the \n",
      "        # Accuracies in validation and test mini batches\n",
      "        index = Tensor.lscalar() # mini-batch index\n",
      "        train_mini_batch = theano.function(\n",
      "            [index], cost, updates=updates,\n",
      "            givens={\n",
      "                self.x: training_x[index*self.mini_batch_size:(index+1)*self.mini_batch_size],\n",
      "                self.y: training_y[index*self.mini_batch_size:(index+1)*self.mini_batch_size]\n",
      "            })\n",
      "        validation_mb_accuracy = theano.function(\n",
      "            [index], self.layers[-1].accuracy(self.y),\n",
      "            givens={\n",
      "                self.x: validation_x[index*self.mini_batch_size:(index+1)*self.mini_batch_size],\n",
      "                self.y: validation_y[index*self.mini_batch_size:(index+1)*self.mini_batch_size]\n",
      "            })\n",
      "        test_mb_accuracy = theano.function(\n",
      "            [index], self.layers[-1].accuracy(self.y),\n",
      "            givens={\n",
      "                self.x: test_x[index*self.mini_batch_size:(index+1)*self.mini_batch_size],\n",
      "                self.y: test_y[index*self.mini_batch_size:(index+1)*self.mini_batch_size]\n",
      "            })\n",
      "        self.test_mb_predictions = theano.function(\n",
      "            [index], self.layers[-1].y_out,\n",
      "            givens={\n",
      "                self.x : test_x[index*self.mini_batch_size:(index+1)*self.mini_batch_size]\n",
      "            })\n",
      "        \n",
      "        # Perform the actual training\n",
      "#         validation_accuracies = []\n",
      "#         test_accuracies = []\n",
      "        best_validation_accuracy = 0.0\n",
      "        best_iteration = 0\n",
      "        for epoch in xrange(epochs):\n",
      "            for mini_batch_index in xrange(num_training_batches):\n",
      "                iteration = num_training_batches * epoch + mini_batch_index\n",
      "#                 if iteration % 500==0:\n",
      "#                     print 'Training Mini-batch number {0}'.format(iteration)\n",
      "                cost_xy = train_mini_batch(mini_batch_index)\n",
      "                if (iteration+1) % num_training_batches == 0:\n",
      "                    validation_accuracy = np.mean(\n",
      "                        [validation_mb_accuracy(num) for num in xrange(num_validation_batches)])\n",
      "#                     validation_accuracies.append(validation_accuracy)\n",
      "#                     print 'Epoch {0}: validation accuracy {1:.2%}'.format(\n",
      "#                             epoch, validation_accuracy)\n",
      "#                     if test_data:\n",
      "#                         test_accuracy = np.mean(\n",
      "#                             [test_mb_accuracy(num) for num in xrange(num_test_batches)])\n",
      "#                         test_accuracies.append(test_accuracy)\n",
      "                    if validation_accuracy >= best_validation_accuracy:\n",
      "#                         print 'This is the best validation accuracy to date'\n",
      "                        best_validation_accuracy = validation_accuracy\n",
      "                        best_iteration = iteration\n",
      "                        if test_data:\n",
      "                            test_accuracy = np.mean(\n",
      "                            [test_mb_accuracy(num) for num in xrange(num_test_batches)])\n",
      "#                             print 'Corresponding Test Accuracy:{0}'.format(test_accuracy)\n",
      "        print \"Finished Training Network\"\n",
      "        print 'Best Validation Accuracy of {0:.2%} acheived at iteration {1}'.format(\n",
      "                best_validation_accuracy, best_iteration)\n",
      "        print 'Corresponding test accuracy is {0:.2%}'.format(test_accuracy)\n",
      "        return best_validation_accuracy, \\\n",
      "    test_accuracy,[self.test_mb_predictions(num) for num in xrange(num_test_batches)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class ConvPoolLayer(object):\n",
      "    '''\n",
      "    Creates a combination of Convolutional Layer and Max Pooling\n",
      "    Note : A better implementation might treat them separately.\n",
      "    '''\n",
      "    def __init__(self, filter_shape, image_shape, poolsize=(2, 2), activation_fn=sigmoid):\n",
      "        ''' \n",
      "        Initializes the layer with the following things\n",
      "        'filter_shape' - a tuple (length-4) that consists of the number of filters,\n",
      "                        number of feature maps, filter height, filter width\n",
      "        'image_shape' - a tuple (length-4) that consists of the 'mini_batch_size',\n",
      "                        number of feature maps, image height, image width\n",
      "        'poolsize'    - a tuple (length-2) that consists of y and x pooling sizez\n",
      "        '''\n",
      "        self.filter_shape = filter_shape\n",
      "        self.image_shape = image_shape\n",
      "        self.activation_fn = activation_fn\n",
      "        self.poolsize = poolsize\n",
      "        # Initialize weights and biases\n",
      "        ''' n_out = num_features*height*width/(poolsize_x * poolsize_y)\n",
      "            Ex: A Convolutional layer with 20 filters and 24 x 24 local receptive field\n",
      "            with a 2 x2 max pool unit gives the output as 20 x 12 x 12\n",
      "        '''\n",
      "        n_out = (filter_shape[0]*np.prod(filter_shape[2:]))/np.prod(poolsize)\n",
      "        self.weights = theano.shared(\n",
      "                        np.asarray(\n",
      "                            np.random.normal(\n",
      "                                loc=0.0, scale=np.sqrt(1.0/n_out), size=(filter_shape)),\n",
      "                            dtype=theano.config.floatX),\n",
      "                        name='weights', borrow=True)\n",
      "        self.biases = theano.shared(\n",
      "                        np.asarray(\n",
      "                            np.random.normal(\n",
      "                                loc=0.0, scale=1.0, size=(filter_shape[0], )),\n",
      "                            dtype=theano.config.floatX),\n",
      "                        name='biases', borrow=True)\n",
      "        self.parameters = [ self.weights, self.biases]\n",
      "        \n",
      "    def set_input(self, inpt, input_dropout, mini_batch_size):\n",
      "        '''\n",
      "        Sets the input for CONV Layer, by reshaping it image_shape format\n",
      "        sets the output by forward pass using  the 'activation_fn'. \n",
      "        NO DROPOUT IN COVOLUTIONAL LAYER\n",
      "        '''\n",
      "        self.inpt = inpt.reshape(self.image_shape)\n",
      "        conv_out = conv.conv2d(\n",
      "                input=self.inpt, filters = self.weights, filter_shape=self.filter_shape,\n",
      "                image_shape = self.image_shape)\n",
      "        pooled_out = downsample.max_pool_2d(\n",
      "                input=conv_out, ds=self.poolsize, ignore_border=True)\n",
      "        self.output = self.activation_fn(\n",
      "                    pooled_out + self.biases.dimshuffle('x', 0, 'x', 'x'))\n",
      "        self.output_dropout = self.output # NO drop out in convolutional layers"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class FullyConnectedLayer(object):\n",
      "    \n",
      "    def __init__(self, n_in, n_out, activation_fn=sigmoid, p_dropout=0.0):\n",
      "        ''' \n",
      "        Initializes the layer with 'n_in' inputs and 'n_out' output connections\n",
      "        with the provided 'activation_fn' and 'p_dropout' rate\n",
      "        '''\n",
      "        self.n_in = n_in\n",
      "        self.n_out = n_out\n",
      "        self.activation_fn = activation_fn\n",
      "        self.p_dropout = p_dropout\n",
      "        # Initialize weights and biases\n",
      "        self.weights = theano.shared(\n",
      "                        np.asarray(\n",
      "                            np.random.normal(\n",
      "                                loc=0.0, scale=np.sqrt(1.0/n_out), size=(n_in, n_out)),\n",
      "                            dtype=theano.config.floatX),\n",
      "                        name='weights', borrow=True)\n",
      "        self.biases = theano.shared(\n",
      "                        np.asarray(\n",
      "                            np.random.normal(\n",
      "                                loc=0.0, scale=1.0, size=(n_out, )),\n",
      "                            dtype=theano.config.floatX),\n",
      "                        name='biases', borrow=True)\n",
      "        self.parameters = [ self.weights, self.biases]\n",
      "        \n",
      "    def set_input(self, inpt, input_dropout, mini_batch_size):\n",
      "        '''\n",
      "        Sets the input for the FC Layer, by reshaping it matrix of size\n",
      "        'mini_batch_size' x 'n_in', sets the output by forward pass using \n",
      "        the 'activation_fn'. 'input_dropout' and 'output_dropout' are set using \n",
      "        the dropout layer prescribed earlier.\n",
      "        '''\n",
      "        self.inpt = inpt.reshape((mini_batch_size, self.n_in))\n",
      "        self.output = self.activation_fn(\n",
      "                    ( 1 - self.p_dropout)* Tensor.dot(self.inpt, self.weights) + self.biases)\n",
      "        self.y_out = Tensor.argmax(self.output, axis=1)\n",
      "        self.input_dropout = dropout_layer(\n",
      "                input_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout)\n",
      "        self.output_dropout = self.activation_fn(\n",
      "                Tensor.dot(self.input_dropout, self.weights) + self.biases)\n",
      "        \n",
      "    def accuracy(self, y):\n",
      "        '''\n",
      "        Returns the accuracy for the mini-batch\n",
      "        '''\n",
      "        return Tensor.mean(Tensor.eq(y , self.y_out))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class SoftMaxLayer(object):\n",
      "    \n",
      "    def __init__(self, n_in, n_out, p_dropout=0.0):\n",
      "        ''' \n",
      "        Initializes the layer with 'n_in' inputs, 'n_out' output connections\n",
      "        ad 'p_dropout' rate\n",
      "        '''\n",
      "        self.n_in = n_in\n",
      "        self.n_out = n_out\n",
      "        self.p_dropout = p_dropout\n",
      "        # Initialize weights and biases\n",
      "        self.weights = theano.shared(\n",
      "                        np.zeros((n_in, n_out), dtype=theano.config.floatX),\n",
      "                        name='weights', borrow=True)\n",
      "        self.biases = theano.shared(\n",
      "                        np.zeros((n_out,), dtype=theano.config.floatX),\n",
      "                        name='biases', borrow=True)\n",
      "        self.parameters = [ self.weights, self.biases]\n",
      "        \n",
      "    def set_input(self, inpt, input_dropout, mini_batch_size):\n",
      "        '''\n",
      "        Sets the input for the Spftmax Layer Layer, by reshaping it matrix of size\n",
      "        'mini_batch_size' x 'n_in', sets the output by forward pass using \n",
      "        the 'activation_fn'. 'input_dropout' and 'output_dropout' are set using \n",
      "        the dropout layer prescribed earlier.\n",
      "        '''\n",
      "        self.inpt = inpt.reshape((mini_batch_size, self.n_in))\n",
      "        self.output = softmax((1 - self.p_dropout)* Tensor.dot(self.inpt, self.weights)\n",
      "                              + self.biases)\n",
      "        self.y_out = Tensor.argmax(self.output, axis=1)\n",
      "        self.input_dropout = dropout_layer(\n",
      "                input_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout)\n",
      "        self.output_dropout = softmax(Tensor.dot(self.input_dropout, self.weights)\n",
      "                                      + self.biases)\n",
      "    \n",
      "    def cost(self, net):\n",
      "        '''\n",
      "        Returns the log-likelihood cost.\n",
      "        '''\n",
      "        return -Tensor.mean(Tensor.log(self.output_dropout)[Tensor.arange(\n",
      "                                                    net.y.shape[0]), net.y])\n",
      "    \n",
      "    def accuracy(self, y):\n",
      "        '''\n",
      "        Returns the accuracy for the mini-batch\n",
      "        '''\n",
      "        return Tensor.mean(Tensor.eq(y , self.y_out))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def size(data):\n",
      "    '''\n",
      "    Returns the size of the dataset 'data'\n",
      "    '''\n",
      "    return data[0].get_value(borrow=True).shape[0]\n",
      "\n",
      "\n",
      "def dropout_layer(layer, p_dropout):\n",
      "    '''\n",
      "    Drop out functionality using theano\n",
      "    '''\n",
      "    srng = shared_randomstreams.RandomStreams(\n",
      "        np.random.RandomState(0).randint(999999))\n",
      "    mask = srng.binomial(n=1, p=1-p_dropout, size=layer.shape)\n",
      "    return layer * Tensor.cast(mask, theano.config.floatX)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "epochs = 60\n",
      "mini_batch_size = 10\n",
      "eta = 0.1\n",
      "training_data, validation_data, test_data = load_data_shared()\n",
      "training_data[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "(<CudaNdarrayType(float32, matrix)>, Elemwise{Cast{int32}}.0)"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "net = Network([ConvPoolLayer(image_shape=(mini_batch_size, 1 , 28, 28),\n",
      "                             filter_shape=(20, 1, 5, 5),\n",
      "                             poolsize=(2,2)),\n",
      "               FullyConnectedLayer(n_in=20*12*12, n_out=100),\n",
      "               SoftMaxLayer(n_in=100, n_out=10)], mini_batch_size)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bva, rta, accuracies = net.SGD(training_data, epochs, mini_batch_size, eta,\n",
      "            validation_data, test_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Finished Training Network\n",
        "Best Validation Accuracy of 98.77% acheived at iteration 184999\n",
        "Corresponding test accuracy is 98.83%\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "(1000, 2, 4)"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "epochs_list = [10, 20, 30, 40, 50, 60]\n",
      "best_validation_accuracies=[]\n",
      "req_test_accuracies = []\n",
      "for epochs in epochs_list:\n",
      "    print 'Training net for epochs:{0}'.format(epochs)\n",
      "    bva, rta = net.SGD(training_data, epochs, mini_batch_size, eta,\n",
      "            validation_data, test_data)\n",
      "    best_validation_accuracies.append(bva)\n",
      "    req_test_accuracies.append(rta)    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training net for epochs:10\n",
        "Finished Training Network"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Best Validation Accuracy of 98.21% acheived at iteration 49999\n",
        "Corresponding test accuracy is 0.00%\n",
        "Training net for epochs:20\n",
        "Finished Training Network"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Best Validation Accuracy of 98.70% acheived at iteration 94999\n",
        "Corresponding test accuracy is 0.00%\n",
        "Training net for epochs:30\n",
        "Finished Training Network"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Best Validation Accuracy of 98.73% acheived at iteration 144999\n",
        "Corresponding test accuracy is 0.00%\n",
        "Training net for epochs:40\n",
        "Finished Training Network"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Best Validation Accuracy of 98.75% acheived at iteration 199999\n",
        "Corresponding test accuracy is 0.00%\n",
        "Training net for epochs:50\n",
        "Finished Training Network"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Best Validation Accuracy of 98.75% acheived at iteration 139999\n",
        "Corresponding test accuracy is 0.00%\n",
        "Training net for epochs:60\n",
        "Finished Training Network"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Best Validation Accuracy of 98.74% acheived at iteration 299999\n",
        "Corresponding test accuracy is 0.00%\n"
       ]
      }
     ],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def plot_overlay(test_accuracy, validation_accuracy, x_label, x_units, y_min, y_max):\n",
      "    fig = plt.figure()\n",
      "    ax = fig.add_subplot(111)\n",
      "    ax.plot(x_units,\n",
      "            [va for va in validation_accuracy], 'o-',\n",
      "            color = '#2A6EA6', label=\"Best Accuracy on Validation Data\")\n",
      "    ax.plot(x_units,\n",
      "            [ta for ta in test_accuracy], '^-',\n",
      "            color='#FFA933', label=\"Best Accuracy on Test Data\")\n",
      "    ax.grid(True)\n",
      "    ax.set_xlabel(x_label)\n",
      "    ax.set_title('Best Accuracies on  Data')\n",
      "    ax.set_ylim([y_min, y_max])\n",
      "    plt.legend(loc='lower right')\n",
      "    plt.show()\n",
      "\n",
      "def plot_accuracy(accuracy, x_units, title, xlabel, ylabel, y_min, y_max):\n",
      "    fig = plt.figure()\n",
      "    ax = fig.add_subplot(111)\n",
      "    ax.plot([x for x in x_units],\n",
      "            [a for a in accuracy], 'o-',\n",
      "            color='#FFA933', label=ylabel)\n",
      "    ax.set_ylim([y_min, y_max])\n",
      "    ax.grid(True)\n",
      "    ax.set_xlabel(xlabel)\n",
      "    ax.set_title(title)\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 113
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_accuracy(best_validation_accuracies, epochs_list, 'Best Accuracy on Validation Data', 'Epochs', \n",
      "              'Best Validation Accuracy', 0.98, 0.99)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 105
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_accuracy(req_test_accuracies, epochs_list, 'Best Accuracy on Test Data', 'Epochs', \n",
      "              'Best Test Accuracy', 0.98, 0.99)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 102
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_overlay(req_test_accuracies, best_validation_accuracies,epochs_list, 0.98, 0.99)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 108
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "epochs = 60\n",
      "etas = [ 0.01, 0.03, 0.1, 0.3, 1.0]\n",
      "best_validation_accuracies=[]\n",
      "req_test_accuracies = []\n",
      "for eta in etas:\n",
      "    print 'Training net for eta:{0}'.format(eta)\n",
      "    bva, rta = net.SGD(training_data, epochs, mini_batch_size, eta,\n",
      "            validation_data, test_data)\n",
      "    best_validation_accuracies.append(bva)\n",
      "    req_test_accuracies.append(rta)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training net for eta:0.01\n",
        "Finished Training Network"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Best Validation Accuracy of 98.75% acheived at iteration 299999\n",
        "Corresponding test accuracy is 0.00%\n",
        "Training net for eta:0.03\n",
        "Finished Training Network"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Best Validation Accuracy of 98.75% acheived at iteration 299999\n",
        "Corresponding test accuracy is 0.00%\n",
        "Training net for eta:0.1\n",
        "Finished Training Network"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Best Validation Accuracy of 98.74% acheived at iteration 299999\n",
        "Corresponding test accuracy is 0.00%\n",
        "Training net for eta:0.3\n",
        "Finished Training Network"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Best Validation Accuracy of 98.76% acheived at iteration 264999\n",
        "Corresponding test accuracy is 0.00%\n",
        "Training net for eta:1.0\n",
        "Finished Training Network"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Best Validation Accuracy of 98.76% acheived at iteration 299999\n",
        "Corresponding test accuracy is 0.00%\n"
       ]
      }
     ],
     "prompt_number": 109
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_accuracy(best_validation_accuracies, etas, 'Best Accuracy on Validation Data', 'Etas', \n",
      "              'Best Validation Accuracy', 0.98, 0.99)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 114
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_accuracy(req_test_accuracies, etas, 'Best Accuracy on Test Data', 'Etas', \n",
      "              'Best Test Accuracy', 0.98, 0.99)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 115
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_overlay(req_test_accuracies, best_validation_accuracies,etas, 0.98, 0.99)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 116
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "normal_net = Network([\n",
      "        FullyConnectedLayer(n_in=784, n_out=100),\n",
      "        SoftMaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
      "bva, rta = normal_net.SGD(training_data, epochs, mini_batch_size, 0.1, \n",
      "            validation_data, test_data)\n",
      "print 'Best Validation Accuracy without Conv Nets:{0}'.format(bva)\n",
      "print \"The corresponding Test Accuracy:{0}\".format(rta)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Finished Training Network\n",
        "Best Validation Accuracy of 97.91% acheived at iteration 299999\n",
        "Corresponding test accuracy is 0.00%\n",
        "Best Validation Accuracy without Conv Nets:0.9791\n",
        "The corresponding Test Accuracy:0.979\n"
       ]
      }
     ],
     "prompt_number": 118
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "convnet_nfc = Network([ConvPoolLayer(image_shape=(mini_batch_size, 1 , 28, 28),\n",
      "                             filter_shape=(20, 1, 5, 5),\n",
      "                             poolsize=(2,2)),\n",
      "               SoftMaxLayer(n_in=20*12*12, n_out=10)], mini_batch_size)\n",
      "bva, rta = convnet_nfc.SGD(training_data, epochs, mini_batch_size, 0.1, \n",
      "            validation_data, test_data)\n",
      "print 'Best Validation Accuracy for Conv Nets without FC:{0}'.format(bva)\n",
      "print \"The corresponding Test Accuracy:{0}\".format(rta)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Finished Training Network\n",
        "Best Validation Accuracy of 98.64% acheived at iteration 194999\n",
        "Corresponding test accuracy is 98.52%\n",
        "Best Validation Accuracy for Conv Nets without FC:0.9864\n",
        "The corresponding Test Accuracy:0.9852\n"
       ]
      }
     ],
     "prompt_number": 121
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Adding Another Convolution Layer'\n",
      "net = Network([ConvPoolLayer(image_shape=(mini_batch_size, 1 , 28, 28),\n",
      "                             filter_shape=(20, 1, 5, 5),\n",
      "                             poolsize=(2,2)),\n",
      "               ConvPoolLayer(image_shape=(mini_batch_size, 20 , 12, 12),\n",
      "                             filter_shape=(40, 20, 5, 5),\n",
      "                             poolsize=(2,2)),\n",
      "               FullyConnectedLayer(n_in=40*4*4, n_out=100),\n",
      "               SoftMaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
      "eta = 0.1\n",
      "epochs = 60\n",
      "mini_batch_size = 10\n",
      "bva, rta = convnet_nfc.SGD(training_data, epochs, mini_batch_size, eta, \n",
      "            validation_data, test_data)\n",
      "print 'Best Validation Accuracy with 2 Conv Nets:{0}'.format(bva)\n",
      "print \"The corresponding Test Accuracy:{0}\".format(rta)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Adding Another Convolution Layer\n",
        "Finished Training Network"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Best Validation Accuracy of 98.53% acheived at iteration 219999\n",
        "Corresponding test accuracy is 98.59%\n",
        "Best Validation Accuracy with 2 Conv Nets:0.9853\n",
        "The corresponding Test Accuracy:0.9859\n"
       ]
      }
     ],
     "prompt_number": 122
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Using RELU as activation function for Conv Layers'\n",
      "epochs = 60\n",
      "mini_batch_size = 10\n",
      "eta = 0.1\n",
      "net = Network([ConvPoolLayer(image_shape=(mini_batch_size, 1 , 28, 28),\n",
      "                             filter_shape=(20, 1, 5, 5),\n",
      "                             poolsize=(2,2), activation_fn = ReLu),\n",
      "               FullyConnectedLayer(n_in=20*12*12, n_out=100),\n",
      "               SoftMaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
      "\n",
      "bva, rta = net.SGD(training_data, epochs, mini_batch_size, eta, \n",
      "            validation_data, test_data)\n",
      "print 'Best Validation Accuracy with Conv Nets using ReLU :{0}'.format(bva)\n",
      "print \"The corresponding Test Accuracy:{0}\".format(rta)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using RELU as activation function for Conv Layers\n",
        "Finished Training Network"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Best Validation Accuracy of 98.78% acheived at iteration 299999\n",
        "Corresponding test accuracy is 98.92%\n",
        "Best Validation Accuracy with Conv Nets using ReLU :0.9878\n",
        "The corresponding Test Accuracy:0.9892\n"
       ]
      }
     ],
     "prompt_number": 124
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "epochs = 60\n",
      "etas = [ 0.01, 0.03, 0.1, 0.3, 1.0]\n",
      "best_validation_accuracies=[]\n",
      "req_test_accuracies = []\n",
      "\n",
      "net = Network([ConvPoolLayer(image_shape=(mini_batch_size, 1 , 28, 28),\n",
      "                             filter_shape=(20, 1, 5, 5),\n",
      "                             poolsize=(2,2), activation_fn = ReLu),\n",
      "               FullyConnectedLayer(n_in=20*12*12, n_out=100),\n",
      "               SoftMaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
      "for eta in etas:\n",
      "    print 'Training net for eta:{0}'.format(eta)\n",
      "    bva, rta = net.SGD(training_data, epochs, mini_batch_size, eta,\n",
      "            validation_data, test_data)\n",
      "    best_validation_accuracies.append(bva)\n",
      "    req_test_accuracies.append(rta)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training net for eta:0.01\n",
        "Finished Training Network"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Best Validation Accuracy of 98.38% acheived at iteration 284999\n",
        "Corresponding test accuracy is 98.49%\n",
        "Training net for eta:0.03\n",
        "Finished Training Network"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Best Validation Accuracy of 98.67% acheived at iteration 244999\n",
        "Corresponding test accuracy is 98.61%\n",
        "Training net for eta:0.1\n",
        "Finished Training Network"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Best Validation Accuracy of 98.84% acheived at iteration 289999\n",
        "Corresponding test accuracy is 98.94%\n",
        "Training net for eta:0.3\n",
        "Finished Training Network"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Best Validation Accuracy of 98.87% acheived at iteration 299999\n",
        "Corresponding test accuracy is 98.96%\n",
        "Training net for eta:1.0\n",
        "Finished Training Network"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Best Validation Accuracy of 98.91% acheived at iteration 224999\n",
        "Corresponding test accuracy is 98.98%\n"
       ]
      }
     ],
     "prompt_number": 128
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_accuracy(best_validation_accuracies, etas, 'Best Accuracy on Validation Data using ReLU', 'Etas', \n",
      "              'Best Validation Accuracy', 0.95, 0.99)\n",
      "\n",
      "plot_accuracy(req_test_accuracies, etas, 'Best Accuracy on Test Data using ReLU', 'Etas', \n",
      "              'Best Test Accuracy', 0.98, 0.99)\n",
      "\n",
      "plot_overlay(req_test_accuracies, best_validation_accuracies,etas, 0.98, 0.99)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 129
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Recitified Linear Units with Another Convolution Layer'\n",
      "\n",
      "net = Network([ConvPoolLayer(image_shape=(mini_batch_size, 1 , 28, 28),\n",
      "                             filter_shape=(20, 1, 5, 5),\n",
      "                             poolsize=(2,2), activation_fn=ReLu),\n",
      "               ConvPoolLayer(image_shape=(mini_batch_size, 20 , 12, 12),\n",
      "                             filter_shape=(40, 20, 5, 5),\n",
      "                             poolsize=(2,2), activation_fn=ReLu),\n",
      "               FullyConnectedLayer(n_in=40*4*4, n_out=100),\n",
      "               SoftMaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
      "eta = 0.1\n",
      "epochs = 60\n",
      "mini_batch_size = 10\n",
      "bva, rta = convnet_nfc.SGD(training_data, epochs, mini_batch_size, eta, \n",
      "            validation_data, test_data)\n",
      "print 'Best Validation Accuracy with 2 Conv Nets:{0}'.format(bva)\n",
      "print \"The corresponding Test Accuracy:{0}\".format(rta)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Recitified Linear Units with Another Convolution Layer\n",
        "Finished Training Network"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Best Validation Accuracy of 98.52% acheived at iteration 234999\n",
        "Corresponding test accuracy is 98.59%\n",
        "Best Validation Accuracy with 2 Conv Nets:0.9852\n",
        "The corresponding Test Accuracy:0.9859\n"
       ]
      }
     ],
     "prompt_number": 126
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Using RELU as activation function for Conv Layers with regularization'\n",
      "epochs = 60\n",
      "mini_batch_size = 10\n",
      "eta = 0.1\n",
      "lmda=0.1\n",
      "net = Network([ConvPoolLayer(image_shape=(mini_batch_size, 1 , 28, 28),\n",
      "                             filter_shape=(20, 1, 5, 5),\n",
      "                             poolsize=(2,2), activation_fn = ReLu),\n",
      "               FullyConnectedLayer(n_in=20*12*12, n_out=100),\n",
      "               SoftMaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
      "\n",
      "bva, rta = net.SGD(training_data, epochs, mini_batch_size, eta, \n",
      "            validation_data, test_data, lmda=lmda)\n",
      "print 'Best Validation Accuracy with Conv Nets using ReLU :{0}'.format(bva)\n",
      "print \"The corresponding Test Accuracy:{0}\".format(rta)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using RELU as activation function for Conv Layers with regularization\n",
        "Finished Training Network"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Best Validation Accuracy of 98.85% acheived at iteration 214999\n",
        "Corresponding test accuracy is 98.81%\n",
        "Best Validation Accuracy with Conv Nets using ReLU :0.9885\n",
        "The corresponding Test Accuracy:0.9881\n"
       ]
      }
     ],
     "prompt_number": 131
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "epochs = 60\n",
      "eta = 1.0\n",
      "lmdas = [ 0.01, 0.03, 0.1, 0.3, 1.0]\n",
      "best_validation_accuracies=[]\n",
      "req_test_accuracies = []\n",
      "for lmda in lmdas:\n",
      "    print 'Training net for eta:{0}'.format(eta)\n",
      "    bva, rta = net.SGD(training_data, epochs, mini_batch_size, eta,\n",
      "            validation_data, test_data,lmda=lmda)\n",
      "    best_validation_accuracies.append(bva)\n",
      "    req_test_accuracies.append(rta)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training net for eta:1.0\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-147-b89573f56093>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'Training net for eta:{0}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     bva, rta = net.SGD(training_data, epochs, mini_batch_size, eta,\n\u001b[0;32m----> 9\u001b[0;31m             validation_data, test_data,lmda=lmda)\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mbest_validation_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbva\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mreq_test_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-120-8aac65ed9f8d>\u001b[0m in \u001b[0;36mSGD\u001b[0;34m(self, training_data, epochs, mini_batch_size, eta, validation_data, test_data, lmda)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m#                 if iteration % 500==0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;31m#                     print 'Training Mini-batch number {0}'.format(iteration)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0mcost_xy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnum_training_batches\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                     validation_accuracy = np.mean(\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    909\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 147
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_accuracy(best_validation_accuracies, etas, 'Best Accuracy on Validation Data using ReLU', 'Lambdas', \n",
      "              'Best Validation Accuracy', 0.95, 0.99)\n",
      "\n",
      "plot_accuracy(req_test_accuracies, etas, 'Best Accuracy on Test Data using ReLU', 'Lambdas', \n",
      "              'Best Test Accuracy', 0.98, 0.99)\n",
      "\n",
      "plot_overlay(req_test_accuracies, best_validation_accuracies,etas, 'Labmdas', 0.98, 0.99)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}